{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def predictpr(fitted, feature_test):\n",
    "    '''\n",
    "    This function predicts the probability of response 1 for the test dataset.\n",
    "    \n",
    "    fitted: fitted classifier\n",
    "    feature_test: feature set in test data\n",
    "    \n",
    "    return: predictions\n",
    "    '''\n",
    "    return fitted.predict_proba(feature_test)[:,1]\n",
    "\n",
    "def knn_score(x_train, y_train, x_test, n, weights = 'uniform', distance_metric = 'minkowski', p=2):\n",
    "    '''\n",
    "    This function builds a KNN classifier.\n",
    "    \n",
    "    x_train: training set with features\n",
    "    y_train: training set with labels\n",
    "    n: number of neighbors\n",
    "    weights: weight function used in prediction.\n",
    "    distance_metric: the distance metric to use\n",
    "    p: Power parameter for the Minkowski metric. \n",
    "    \n",
    "    returns: fitted KNN classifier\n",
    "    '''\n",
    "    if distance_metric == 'minkowski':\n",
    "        knn = KNeighborsClassifier(n_neighbors=n, weights = weights, p=p, metric= distance_metric)\n",
    "        \n",
    "    else:\n",
    "        knn =KNeighborsClassifier(n_neighbors=n, weights = weights, metric= distance_metric)\n",
    "    \n",
    "    knn.fit(x_train, y_train)\n",
    "    \n",
    "    return predictpr(knn, x_test)\n",
    "\n",
    "#can use existing prediction function to predict scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def lr_score(x_train, y_train, x_test, p = 'l1', c = 1.0, solver = 'liblinear', seed=12345):\n",
    "    '''\n",
    "    This function builds a Logistic Regression model.\n",
    "    \n",
    "    x_train: training set with features\n",
    "    y_train: training set with labels\n",
    "    p: penalty (l1 or l2)\n",
    "    c: Inverse of regularization strength; must be a positive float.\n",
    "    solver: Algorithm to use in the optimization problem.\n",
    "    seed: random seed\n",
    "    \n",
    "    returns fitted Logistic Regression\n",
    "    '''\n",
    "    \n",
    "    lr = LogisticRegression(penalty = p, C = c, solver= solver, random_state = seed)\n",
    "    lr.fit(x_train, y_train)\n",
    "    \n",
    "    return predictpr(lr, x_test)\n",
    "\n",
    "\n",
    "#can use existing prediction function to predict scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def linsvc_score(x_train, y_train, x_test, p = 'l2', c = 1.0, seed = 12345):\n",
    "    '''\n",
    "    This function builds a fitted linear SVC\n",
    "    \n",
    "    x_train: training set with features\n",
    "    y_train: training set with labels\n",
    "    p: penalty (l2)\n",
    "    c: Penalty parameter C of the error term\n",
    "    seed: random seed\n",
    "    \n",
    "    returns fitted linear SVC\n",
    "    '''\n",
    "    \n",
    "    lsvc = LinearSVC(penalty = p, C=c, seed=seed)\n",
    "    lsvc.fit(x_train, y_train)\n",
    "    \n",
    "    return lsvc.decision_function(x_test)\n",
    "\n",
    "def svm_score(fitted_lsvc, x_test):\n",
    "    '''\n",
    "    This function uses a test set to make scores using Linear SVC.\n",
    "    \n",
    "    fitted_lsvc: fitted Linear SVC\n",
    "    x_test: test set with features\n",
    "    \n",
    "    returns predictions\n",
    "    '''\n",
    "    return fitted_lsvc.decision_function(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "\n",
    "def accuracy_at_threshold(y_test, pred_scores, thresh =0.5):\n",
    "    '''\n",
    "    This function calculates that accuracy of model given a threshold\n",
    "\n",
    "    y_test: real labels\n",
    "    pred: prediction scores\n",
    "    threshold: threshold for predictions\n",
    "\n",
    "    returns accuracy \n",
    "    '''\n",
    "    pred_one = [1 if x > thresh else 0 for x in pred_scores]\n",
    "    \n",
    "    return metrics.accuracy_score(y_test, pred_one)\n",
    "\n",
    "def build_cmatrix(y_test, pred_scores, threshold):\n",
    "    '''\n",
    "    This function builds a confusion matrix for a given threshold\n",
    "    \n",
    "    pred_scores: prediction scores\n",
    "    y_test: real labels\n",
    "    threshold: threshold for predictions\n",
    "    \n",
    "    returns tuple (true_negatives, false_positive, false_negatives, true_positives)\n",
    "    '''\n",
    "    pred = [1 if x > threshold else 0 for x in pred_scores]\n",
    "    \n",
    "    cmatrix = confusion_matrix(y_test, pred)\n",
    "    \n",
    "    true_negatives, false_positive, false_negatives, true_positives = cmatrix.ravel()\n",
    "\n",
    "    return (true_negatives, false_positive, false_negatives, true_positives)\n",
    "\n",
    "def precision_at_threshold(y_test, pred_scores, thresh =0.5):\n",
    "    '''\n",
    "    This function calculates precision of model given a threshold\n",
    "\n",
    "    y_test: real labels\n",
    "    pred_scores: prediction scores\n",
    "    threshold: threshold for predictions\n",
    "\n",
    "    returns precision\n",
    "    '''\n",
    "    pred_one = [1 if x > thresh else 0 for x in pred_scores]\n",
    "    \n",
    "    return metrics.precision_score(y_test, pred_one)\n",
    "\n",
    "def recall_at_threshold(y_test, pred_scores, thresh =0.5):\n",
    "    '''\n",
    "    This function calculates recall of model given a threshold\n",
    "\n",
    "    y_test: real labels\n",
    "    pred_scores: prediction scores\n",
    "    threshold: threshold for predictions\n",
    "\n",
    "    returns recall\n",
    "    '''\n",
    "    pred_one = [1 if x > thresh else 0 for x in pred_scores]\n",
    "    \n",
    "    return metrics.recall_score(y_test, pred_one)\n",
    "\n",
    "def f1_at_threshold(y_test, pred_scores, thresh =0.5):\n",
    "    '''\n",
    "    This function calculates that accuracy of model given a threshold\n",
    "\n",
    "    y_test: real labels\n",
    "    pred_scores: prediction scores\n",
    "    threshold: threshold for predictions\n",
    "\n",
    "    returns f1 score \n",
    "    '''\n",
    "    pred_one = [1 if x > thresh else 0 for x in pred_scores]\n",
    "    \n",
    "    return metrics.f1_score(y_test, pred_one)\n",
    "\n",
    "def auc_roc(y_test, pred_scores):\n",
    "    '''\n",
    "    This function calculates the area under the ROC curve\n",
    "    \n",
    "    y_test: real labels\n",
    "    pred_scores: prediction scores\n",
    "    \n",
    "    returns auc\n",
    "    '''\n",
    "    \n",
    "    return metrics.roc_auc_score(y_test, pred_scores)\n",
    "\n",
    "def plot_precision_recall(y_test, pred_scores):\n",
    "    '''\n",
    "    This function plots the precision recall curve\n",
    "    \n",
    "    y_test: true labels\n",
    "    pred_scores: predicted scores\n",
    "    \n",
    "    return: none\n",
    "    '''\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, pred_scores)\n",
    "    plt.pyplot.plot(recall, precision, marker='.')\n",
    "    plt.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_pipeline as pp\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "file = './data/projects_2012_2013.csv'\n",
    "df = pp.load_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for hw3 specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_datetime(df, tgt_col):\n",
    "    '''\n",
    "    Turn column values into datetime\n",
    "    \n",
    "    df: dataframe\n",
    "    tgt_col: column name\n",
    "    \n",
    "    returns pandas series with datetime\n",
    "    '''\n",
    "    return pd.to_datetime(df[tgt_col])\n",
    "\n",
    "def create_label(df, pred_time = 60, pred_unit = 'day'):\n",
    "    '''\n",
    "    This function creates a label column in the dataset given a time horizon\n",
    "    \n",
    "    df: dataframe\n",
    "    pred_time: prediction time horizon\n",
    "    pred_unit: unit of time of prediction (i.e. day or month or year)\n",
    "    \n",
    "    return dataframe\n",
    "    '''\n",
    "    def label(row, pred_time, pred_unit):\n",
    "        if 'day' in pred_unit:\n",
    "            diff = pred_time\n",
    "        elif 'month' in pred_unit:\n",
    "            diff = pred_time * 30 #convert to days\n",
    "        elif 'year' in pred_unit:\n",
    "            diff = pred_time * 365 #convert to days\n",
    "            \n",
    "        if (row.datefullyfunded - row.date_posted).days <= diff:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['label'] = df.apply(lambda row: label(row, pred_time, pred_unit), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def single_train_test_set(df, feature_cols, label_col, split_col, train_start, train_end, test_end, pred_time, pred_unit = 'day'):\n",
    "    '''\n",
    "    This function builds a single temporal training and test set\n",
    "    \n",
    "    df: dataframe with all data\n",
    "    feature_cols: list of feature columns\n",
    "    label_col: label column string\n",
    "    split_col: column with date to split on\n",
    "    train_start: date to begin training data\n",
    "    train_end: date to end training data\n",
    "    test_end: date to end test data\n",
    "    pred_time: time horizon for predictions (day, month or year)\n",
    "    \n",
    "    returns tuple of x_train, y_train, x_test, y_test\n",
    "    '''\n",
    "    \n",
    "    if 'day' in pred_unit:\n",
    "        actual_train_end = train_end - dt.timedelta(days=pred_time)\n",
    "    elif 'month' in pred_unit:\n",
    "        actual_train_end = train_end - md.monthdelta(pred_time)\n",
    "    elif 'year' in pred_unit.contains:\n",
    "        actual_train_end = dt.datetime(train_end.year - pred_time, train_end.month, train_end.day)\n",
    "        \n",
    "    training_set = df[(df[split_col] >= train_start) & (df[split_col] <= actual_train_end)]\n",
    "    \n",
    "    test_set = df[(df[split_col] >= train_end) & (df[split_col] <= test_end)]\n",
    "    \n",
    "    return (training_set[feature_cols], training_set[label_col], test_set[feature_cols], test_set[label_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert date columns to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date_posted = pp.col_datetime(df, 'date_posted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.datefullyfunded = pp.col_datetime(df,'datefullyfunded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pp.create_label(df, pred_time=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_metro</th>\n",
       "      <th>school_charter</th>\n",
       "      <th>school_magnet</th>\n",
       "      <th>primary_focus_subject</th>\n",
       "      <th>primary_focus_area</th>\n",
       "      <th>resource_type</th>\n",
       "      <th>poverty_level</th>\n",
       "      <th>grade_level</th>\n",
       "      <th>total_price_including_optional_support</th>\n",
       "      <th>students_reached</th>\n",
       "      <th>eligible_double_your_impact_match</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>urban</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Math &amp; Science</td>\n",
       "      <td>Supplies</td>\n",
       "      <td>highest poverty</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>1498.61</td>\n",
       "      <td>31.0</td>\n",
       "      <td>f</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>urban</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Civics &amp; Government</td>\n",
       "      <td>History &amp; Civics</td>\n",
       "      <td>Books</td>\n",
       "      <td>highest poverty</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>282.47</td>\n",
       "      <td>28.0</td>\n",
       "      <td>t</td>\n",
       "      <td>2012-04-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>urban</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Literacy</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>Technology</td>\n",
       "      <td>high poverty</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>1012.38</td>\n",
       "      <td>56.0</td>\n",
       "      <td>f</td>\n",
       "      <td>2012-01-30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>urban</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>Literacy</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>Books</td>\n",
       "      <td>high poverty</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>175.33</td>\n",
       "      <td>23.0</td>\n",
       "      <td>f</td>\n",
       "      <td>2012-10-11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suburban</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Literacy</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>Technology</td>\n",
       "      <td>high poverty</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>3591.11</td>\n",
       "      <td>150.0</td>\n",
       "      <td>f</td>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  school_metro school_charter school_magnet primary_focus_subject  \\\n",
       "0        urban              f             f           Mathematics   \n",
       "1        urban              f             f   Civics & Government   \n",
       "2        urban              f             f              Literacy   \n",
       "3        urban              f             t              Literacy   \n",
       "4     suburban              f             f              Literacy   \n",
       "\n",
       "    primary_focus_area resource_type    poverty_level    grade_level  \\\n",
       "0       Math & Science      Supplies  highest poverty  Grades PreK-2   \n",
       "1     History & Civics         Books  highest poverty     Grades 3-5   \n",
       "2  Literacy & Language    Technology     high poverty     Grades 3-5   \n",
       "3  Literacy & Language         Books     high poverty  Grades PreK-2   \n",
       "4  Literacy & Language    Technology     high poverty  Grades PreK-2   \n",
       "\n",
       "   total_price_including_optional_support  students_reached  \\\n",
       "0                                 1498.61              31.0   \n",
       "1                                  282.47              28.0   \n",
       "2                                 1012.38              56.0   \n",
       "3                                  175.33              23.0   \n",
       "4                                 3591.11             150.0   \n",
       "\n",
       "  eligible_double_your_impact_match date_posted  label  \n",
       "0                                 f  2013-04-14      1  \n",
       "1                                 t  2012-04-07      1  \n",
       "2                                 f  2012-01-30      0  \n",
       "3                                 f  2012-10-11      1  \n",
       "4                                 f  2013-01-08      0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols=['school_metro','school_charter', 'school_magnet', 'primary_focus_subject', 'primary_focus_area', 'resource_type', 'poverty_level', 'grade_level', 'total_price_including_optional_support', 'students_reached', 'eligible_double_your_impact_match', 'date_posted', 'label']\n",
    "sel = df[feature_cols].copy()\n",
    "sel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify feature columns with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "school_metro\n",
      "primary_focus_subject\n",
      "primary_focus_area\n",
      "resource_type\n",
      "grade_level\n",
      "students_reached\n"
     ]
    }
   ],
   "source": [
    "for x in pp.na_col(df):\n",
    "    if x in feature_cols:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute missing categorical variables with the most frequent, which is a common way to handle missing categorical data without more information. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['school_metro','primary_focus_subject','primary_focus_area','resource_type','grade_level']\n",
    "for x in cat_cols:\n",
    "    sel = pp.na_fill_col(sel, x , pp.most_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute missing numerical variable (students_reached) with the median value because there are outliers affecting the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10       18.0\n",
       "0.25       23.0\n",
       "0.50       30.0\n",
       "0.75      100.0\n",
       "0.90      200.0\n",
       "0.98      700.0\n",
       "1.00    12143.0\n",
       "Name: students_reached, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.students_reached.quantile([0.1, 0.25, 0.5, 0.75, 0.9,0.98,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = pp.na_fill_col(sel, 'students_reached', np.nanmedian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that there are no more missing values in feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in pp.na_col(sel):\n",
    "    if x in feature_cols:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discretize numeric features and then get all dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(30.0, 100.0], (23.0, 30.0], (0.999, 23.0], (100.0, 12143.0]]\n",
       "Categories (4, interval[float64]): [(0.999, 23.0] < (23.0, 30.0] < (30.0, 100.0] < (100.0, 12143.0]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# discretize numeric features\n",
    "bucketdict= {'total_price_including_optional_support': 4, 'students_reached':4}\n",
    "df_discr = pp.feat_mult_disc(sel, bucketdict, qt=True)\n",
    "\n",
    "df_discr.total_price_including_optional_support_binned.unique()\n",
    "df_discr.students_reached_binned.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_binary = list(df_discr.columns)\n",
    "col_to_binary.remove('label')\n",
    "col_to_binary.remove('date_posted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['school_metro',\n",
       " 'school_charter',\n",
       " 'school_magnet',\n",
       " 'primary_focus_subject',\n",
       " 'primary_focus_area',\n",
       " 'resource_type',\n",
       " 'poverty_level',\n",
       " 'grade_level',\n",
       " 'eligible_double_your_impact_match',\n",
       " 'total_price_including_optional_support_binned',\n",
       " 'students_reached_binned']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_to_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_posted</th>\n",
       "      <th>label</th>\n",
       "      <th>school_metro_rural</th>\n",
       "      <th>school_metro_suburban</th>\n",
       "      <th>school_metro_urban</th>\n",
       "      <th>school_charter_f</th>\n",
       "      <th>school_charter_t</th>\n",
       "      <th>school_magnet_f</th>\n",
       "      <th>school_magnet_t</th>\n",
       "      <th>primary_focus_subject_Applied Sciences</th>\n",
       "      <th>...</th>\n",
       "      <th>eligible_double_your_impact_match_f</th>\n",
       "      <th>eligible_double_your_impact_match_t</th>\n",
       "      <th>total_price_including_optional_support_binned_(91.999, 345.81]</th>\n",
       "      <th>total_price_including_optional_support_binned_(345.81, 510.5]</th>\n",
       "      <th>total_price_including_optional_support_binned_(510.5, 752.96]</th>\n",
       "      <th>total_price_including_optional_support_binned_(752.96, 164382.84]</th>\n",
       "      <th>students_reached_binned_(0.999, 23.0]</th>\n",
       "      <th>students_reached_binned_(23.0, 30.0]</th>\n",
       "      <th>students_reached_binned_(30.0, 100.0]</th>\n",
       "      <th>students_reached_binned_(100.0, 12143.0]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-04-07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-10-11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  date_posted  label  school_metro_rural  school_metro_suburban  \\\n",
       "0  2013-04-14      1                   0                      0   \n",
       "1  2012-04-07      1                   0                      0   \n",
       "2  2012-01-30      0                   0                      0   \n",
       "3  2012-10-11      1                   0                      0   \n",
       "4  2013-01-08      0                   0                      1   \n",
       "\n",
       "   school_metro_urban  school_charter_f  school_charter_t  school_magnet_f  \\\n",
       "0                   1                 1                 0                1   \n",
       "1                   1                 1                 0                1   \n",
       "2                   1                 1                 0                1   \n",
       "3                   1                 1                 0                0   \n",
       "4                   0                 1                 0                1   \n",
       "\n",
       "   school_magnet_t  primary_focus_subject_Applied Sciences  \\\n",
       "0                0                                       0   \n",
       "1                0                                       0   \n",
       "2                0                                       0   \n",
       "3                1                                       0   \n",
       "4                0                                       0   \n",
       "\n",
       "                     ...                     \\\n",
       "0                    ...                      \n",
       "1                    ...                      \n",
       "2                    ...                      \n",
       "3                    ...                      \n",
       "4                    ...                      \n",
       "\n",
       "   eligible_double_your_impact_match_f  eligible_double_your_impact_match_t  \\\n",
       "0                                    1                                    0   \n",
       "1                                    0                                    1   \n",
       "2                                    1                                    0   \n",
       "3                                    1                                    0   \n",
       "4                                    1                                    0   \n",
       "\n",
       "   total_price_including_optional_support_binned_(91.999, 345.81]  \\\n",
       "0                                                  0                \n",
       "1                                                  1                \n",
       "2                                                  0                \n",
       "3                                                  1                \n",
       "4                                                  0                \n",
       "\n",
       "   total_price_including_optional_support_binned_(345.81, 510.5]  \\\n",
       "0                                                  0               \n",
       "1                                                  0               \n",
       "2                                                  0               \n",
       "3                                                  0               \n",
       "4                                                  0               \n",
       "\n",
       "   total_price_including_optional_support_binned_(510.5, 752.96]  \\\n",
       "0                                                  0               \n",
       "1                                                  0               \n",
       "2                                                  0               \n",
       "3                                                  0               \n",
       "4                                                  0               \n",
       "\n",
       "   total_price_including_optional_support_binned_(752.96, 164382.84]  \\\n",
       "0                                                  1                   \n",
       "1                                                  0                   \n",
       "2                                                  1                   \n",
       "3                                                  0                   \n",
       "4                                                  1                   \n",
       "\n",
       "   students_reached_binned_(0.999, 23.0]  \\\n",
       "0                                      0   \n",
       "1                                      0   \n",
       "2                                      0   \n",
       "3                                      1   \n",
       "4                                      0   \n",
       "\n",
       "   students_reached_binned_(23.0, 30.0]  \\\n",
       "0                                     0   \n",
       "1                                     1   \n",
       "2                                     0   \n",
       "3                                     0   \n",
       "4                                     0   \n",
       "\n",
       "   students_reached_binned_(30.0, 100.0]  \\\n",
       "0                                      1   \n",
       "1                                      0   \n",
       "2                                      1   \n",
       "3                                      0   \n",
       "4                                      0   \n",
       "\n",
       "   students_reached_binned_(100.0, 12143.0]  \n",
       "0                                         0  \n",
       "1                                         0  \n",
       "2                                         0  \n",
       "3                                         0  \n",
       "4                                         1  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn variables into dummies\n",
    "df_final = pp.feat_binary(df_discr, col_to_binary)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run variations of models: \n",
    "### Decision trees, KNN, Logistic Regression, Linear SVM, Random forests, Bagging, Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [dt.datetime(2012,1,1), dt.datetime(2012,7,1), dt.datetime(2013,1,1), dt.datetime(2013,7,1), dt.datetime(2014,1,1)]\n",
    "pred_time = 60 #days\n",
    "label_col = 'label'\n",
    "split_col = 'date_posted'\n",
    "feature_cols= list(df_final.columns)\n",
    "feature_cols.remove('label')\n",
    "feature_cols.remove('date_posted')\n",
    "seed=12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    {'type': 'Dtree', 'clf': pp.dtree_score, 'criteria': ['entropy', 'gini'], 'depth': [5,10,15],'min_leaf': [300,500,700], 'seed': seed},\n",
    "    {'type': 'LR', 'clf': lr_score, 'p': ['l1','l2'], 'c': [0.1, 1.0, 10.0, 100.0], 'solver': ['liblinear'], 'seed': seed},\n",
    "    {'type': 'SVM', 'clf': linsvc_score, 'p': ['l2'], 'c': [0.1, 1.0, 10.0, 100.0], 'seed': seed},\n",
    "    {'type': 'KNN', 'clf': knn_score, 'n': [5,15], 'weights': ['uniform','distance'], 'distance_metric':['minkowski'], 'p': [1,2]}\n",
    "]\n",
    "\n",
    "thresholds = [0.3, 0.5, 0.7]\n",
    "svm_thresh = [-0.5, 0, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: Dtree, run: 1\n",
      "criteria: entropy, depth: 5, min_leaf: 300, seed: 12345\n",
      "criteria: entropy, depth: 5, min_leaf: 500, seed: 12345\n",
      "criteria: entropy, depth: 5, min_leaf: 700, seed: 12345\n",
      "criteria: entropy, depth: 10, min_leaf: 300, seed: 12345\n",
      "criteria: entropy, depth: 10, min_leaf: 500, seed: 12345\n",
      "criteria: entropy, depth: 10, min_leaf: 700, seed: 12345\n",
      "criteria: entropy, depth: 15, min_leaf: 300, seed: 12345\n",
      "criteria: entropy, depth: 15, min_leaf: 500, seed: 12345\n",
      "criteria: entropy, depth: 15, min_leaf: 700, seed: 12345\n",
      "criteria: gini, depth: 5, min_leaf: 300, seed: 12345\n",
      "criteria: gini, depth: 5, min_leaf: 500, seed: 12345\n",
      "criteria: gini, depth: 5, min_leaf: 700, seed: 12345\n",
      "criteria: gini, depth: 10, min_leaf: 300, seed: 12345\n",
      "criteria: gini, depth: 10, min_leaf: 500, seed: 12345\n",
      "criteria: gini, depth: 10, min_leaf: 700, seed: 12345\n",
      "criteria: gini, depth: 15, min_leaf: 300, seed: 12345\n",
      "criteria: gini, depth: 15, min_leaf: 500, seed: 12345\n",
      "criteria: gini, depth: 15, min_leaf: 700, seed: 12345\n",
      "model: LR, run: 1\n",
      "l1\n",
      "penalty: l1, c: 0.1, solver: liblinear, seed: 12345\n",
      "l1\n",
      "penalty: l1, c: 1.0, solver: liblinear, seed: 12345\n",
      "l1\n",
      "penalty: l1, c: 10.0, solver: liblinear, seed: 12345\n",
      "l1\n",
      "penalty: l1, c: 100.0, solver: liblinear, seed: 12345\n",
      "l2\n",
      "penalty: l2, c: 0.1, solver: liblinear, seed: 12345\n",
      "l2\n",
      "penalty: l2, c: 1.0, solver: liblinear, seed: 12345\n",
      "l2\n",
      "penalty: l2, c: 10.0, solver: liblinear, seed: 12345\n",
      "l2\n",
      "penalty: l2, c: 100.0, solver: liblinear, seed: 12345\n",
      "model: SVM, run: 1\n",
      "penalty: l2, c: 0.1, seed: 12345\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-3965ce5afa87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'penalty: {}, c: {}, seed: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msvm_thresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_at_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-41abc70f570d>\u001b[0m in \u001b[0;36mlinsvc_score\u001b[0;34m(x_train, y_train, x_test, p, c, seed)\u001b[0m\n\u001b[1;32m     16\u001b[0m     '''\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mlsvc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mlsvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'seed'"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(1, len(windows)-1):\n",
    "    train_start = windows[0]\n",
    "    train_end = windows[i]\n",
    "    test_end = windows[i+1]\n",
    "    \n",
    "    #split data\n",
    "    x_train,y_train,x_test,y_test = pp.single_train_test_set(df_final, \n",
    "                                                          feature_cols, \n",
    "                                                          label_col, \n",
    "                                                          split_col, \n",
    "                                                          train_start,\n",
    "                                                          train_end, \n",
    "                                                          test_end, \n",
    "                                                          pred_time=pred_time)\n",
    "    \n",
    "    \n",
    "    #run models\n",
    "    for clf in models:\n",
    "        modeltype = clf['type']\n",
    "        func = clf['clf']\n",
    "        printinfo = 'model: {}, run: {}'.format(modeltype, i)\n",
    "        print(printinfo)\n",
    "        if modeltype == 'Dtree':\n",
    "            seed = clf['seed']\n",
    "            for c in clf['criteria']:\n",
    "                for d in clf['depth']:\n",
    "                    for l in clf['min_leaf']:\n",
    "                        #run model\n",
    "                        info = 'criteria: {}, depth: {}, min_leaf: {}, seed: {}'.format(c, d, l, seed)\n",
    "                        print(info)\n",
    "                        scores = func(x_train, y_train, x_test, criteria = c, depth = d, min_leaf = l, seed=seed)\n",
    "                        for t in thresholds:\n",
    "                            acc = accuracy_at_threshold(y_test, scores, t)\n",
    "                            prec = precision_at_threshold(y_test, scores, t)\n",
    "                            rec = recall_at_threshold(y_test, scores, t)\n",
    "                            f1 = f1_at_threshold(y_test, scores, t)\n",
    "                            auc = auc_roc(y_test,scores)\n",
    "                            tmp = {'train_set_num': i, 'train_start': train_start, 'test_start': train_end,'type': modeltype, \n",
    "                                   'details': info, 'threshold': t, 'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'auc': auc}\n",
    "                            results.append(tmp)\n",
    "                            \n",
    "        elif modeltype == 'LR':\n",
    "            seed = clf['seed']\n",
    "            for p in clf['p']:\n",
    "                for c in clf['c']:\n",
    "                    for s in clf['solver']:\n",
    "                        print(p)\n",
    "                        info = 'penalty: {}, c: {}, solver: {}, seed: {}'.format(p, c, s, seed)\n",
    "                        print(info)\n",
    "                        scores = func(x_train, y_train, x_test, p = p, c = c, solver = s, seed=seed)\n",
    "                        for t in thresholds:\n",
    "                            acc = accuracy_at_threshold(y_test, scores, t)\n",
    "                            prec = precision_at_threshold(y_test, scores, t)\n",
    "                            rec = recall_at_threshold(y_test, scores, t)\n",
    "                            f1 = f1_at_threshold(y_test, scores, t)\n",
    "                            auc = auc_roc(y_test, scores)\n",
    "                            tmp = {'train_set_num': i, 'train_start': train_start, 'test_start': train_end,'type': modeltype, \n",
    "                                   'details': info, 'threshold': t, 'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'auc': auc}\n",
    "                            results.append(tmp)\n",
    "                        \n",
    "        elif modeltype == 'SVM':\n",
    "            seed = clf['seed']\n",
    "            for p in clf['p']:\n",
    "                for c in clf['c']:\n",
    "                    info = 'penalty: {}, c: {}, seed: {}'.format(p, c, seed)\n",
    "                    print(info)\n",
    "                    scores = func(x_train, y_train, x_test, p =p, c=c, seed=seed)\n",
    "                    for t in svm_thresh:\n",
    "                            acc = accuracy_at_threshold(y_test, scores, t)\n",
    "                            prec = precision_at_threshold(y_test, scores, t)\n",
    "                            rec = recall_at_threshold(y_test, scores, t)\n",
    "                            f1 = f1_at_threshold(y_test, scores, t)\n",
    "                            auc = auc_roc(y_test, scores)\n",
    "                            tmp = {'train_set_num': i, 'train_start': train_start, 'test_start': train_end,'type': modeltype, \n",
    "                                   'details': info, 'threshold': t, 'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'auc': auc}\n",
    "                            results.append(tmp)\n",
    "                            \n",
    "        elif modeltype == 'KNN':\n",
    "            for n in clf['n']:\n",
    "                for w in clf['weights']:\n",
    "                    for d in clf['distance_metric']:\n",
    "                        for p in clf['p']:\n",
    "                            info = 'n: {}, weights: {}, distance: {}, p: {}'.format(n, w, d, p)\n",
    "                            print(info)\n",
    "                            scores = func(x_train, y_train, x_test, n = n, weights = w, distance_metric = d, p=p)\n",
    "                            #print(list(scores))\n",
    "                            for t in thresholds:\n",
    "                                acc = accuracy_at_threshold(y_test, scores, t)\n",
    "                                prec = precision_at_threshold(y_test, scores, t)\n",
    "                                rec = recall_at_threshold(y_test, scores, t)\n",
    "                                f1 = f1_at_threshold(y_test, scores, t)\n",
    "                                auc = auc_roc(y_test,scores)\n",
    "                                tmp = {'train_set_num': i, 'train_start': train_start, 'test_start': train_end,'type': modeltype, \n",
    "                                       'details': info, 'threshold': t, 'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'auc': auc}\n",
    "                                results.append(tmp)\n",
    "                    \n",
    "pd.DataFrame(results)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Use temporal validation to split into training/test sets\n",
    "\n",
    "\n",
    "\n",
    "x_train1,y_train1,x_test1,y_test1 = pp.single_train_test_set(df_final, \n",
    "                                                          feature_cols, \n",
    "                                                          label_col, \n",
    "                                                          split_col, \n",
    "                                                          windows[0],\n",
    "                                                          windows[1], \n",
    "                                                          windows[2], \n",
    "                                                          pred_time=60)\n",
    "\n",
    "x_train2, y_train2, x_test2, y_test2 = pp.single_train_test_set(df_final, \n",
    "                                                          feature_cols, \n",
    "                                                          label_col, \n",
    "                                                          split_col, \n",
    "                                                          windows[0],\n",
    "                                                          windows[2], \n",
    "                                                          windows[3], \n",
    "                                                          pred_time=60)\n",
    "\n",
    "x_train3, y_train3, x_test3, y_test3 = pp.single_train_test_set(df_final, \n",
    "                                                          feature_cols, \n",
    "                                                          label_col, \n",
    "                                                          split_col, \n",
    "                                                          windows[0],\n",
    "                                                          windows[3], \n",
    "                                                          windows[4], \n",
    "                                                          pred_time=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66666667, 0.66666667, 1.        , ..., 1.        , 1.        ,\n",
       "       0.66666667])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = knn_score(x_train1, y_train1, x_test1, n=3, p=1)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6534890776699029"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_at_threshold(y_test1, clf, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " ...]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=lr_score(x_train1, y_train1, x_test1, p = 'l1', c = 0.1, solver = 'liblinear', seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80614602, 0.61825945, 0.55738874, ..., 0.82086886, 0.82179345,\n",
       "       0.68883629])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
